{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7080d9a8",
   "metadata": {},
   "source": [
    "# Task-2: Generating Faces\n",
    "\n",
    "This part of the task requires training a face generation model from embeddings. To generate suitable embeddings, I currently plan to use `gaunernst/vit_tiny_patch8_112.arcface_ms1mv3` model, which is a pre-trained ViT, trained upon MS1MV3 dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b23a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3296 training images found.\n",
      "582 validation images found.\n"
     ]
    }
   ],
   "source": [
    "# dataset generation\n",
    "import os\n",
    "import torch\n",
    "import timm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "random.seed(42)  # For reproducibility\n",
    "\n",
    "class FaceDataset(Dataset):\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_files = [f for f in os.listdir(image_dir) if f.endswith('.jpg')]\n",
    "        \n",
    "        self.embedding_model = timm.create_model(\"hf_hub:gaunernst/vit_tiny_patch8_112.arcface_ms1mv3\", pretrained=True).eval()\n",
    "        self.data_config = timm.data.resolve_data_config(self.embedding_model.pretrained_cfg)\n",
    "        self.transform = timm.data.create_transform( **self.data_config, is_training=False)\n",
    "        if not self.image_files:\n",
    "            raise ValueError(f\"No images found in directory: {image_dir}\")\n",
    "        \n",
    "        self.to_latent = transforms.Compose([\n",
    "            transforms.Resize((128, 128)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(0.5, 0.5)\n",
    "            ])\n",
    "        \n",
    "    \n",
    "    def get_embedding(self, image):\n",
    "        with torch.inference_mode():\n",
    "            image = self.transform(image).unsqueeze(0)\n",
    "            embedding = self.embedding_model(image)\n",
    "            embedding = F.normalize(embedding, dim=1)\n",
    "        return embedding.squeeze(0) # From [1, 512] to [512]\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        pil_image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        flip_prob=0.2\n",
    "        if random.random() < flip_prob:\n",
    "            print(f\"Flipping image: {self.image_files[idx]}\")\n",
    "            pil_image = pil_image.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "        \n",
    "        crop_prob=0.15\n",
    "        if random.random() < crop_prob:\n",
    "            print(f\"Cropping image: {self.image_files[idx]}\")\n",
    "            width, height = pil_image.size\n",
    "            left = int(width * 0.05)\n",
    "            top = int(height * 0.05)\n",
    "            right = int(width * 0.95)\n",
    "            bottom = int(height * 0.95)\n",
    "            pil_image = pil_image.crop((left, top, right, bottom))\n",
    "        \n",
    "        embedding = self.get_embedding(pil_image)\n",
    "        \n",
    "        image = self.to_latent(pil_image)\n",
    "        \n",
    "        return {\n",
    "            'image': image,\n",
    "            'embedding': embedding\n",
    "        }\n",
    "    \n",
    "    \n",
    "train_dataset = FaceDataset('data/train')\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = FaceDataset('data/val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(train_dataset.__len__(), \"training images found.\")\n",
    "print(val_dataset.__len__(), \"validation images found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63150b60",
   "metadata": {},
   "source": [
    "## Creating and Training the model.\n",
    "Since I am a bit inexperienced in training diffusion models (as I previously mainly worked in the domain of NLP), I will try to keep the training process as simple and default as possible. The following code is adapted from the [diffusers_training_example.ipynb](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb#scrollTo=67640279-979b-490d-80fe-65673b94ae00), which is a Hugging Face example for training diffusion models.\n",
    "\n",
    "That means, I will be using the classic UNet2Dmodel, to run the diffusion process. \n",
    "\n",
    "* The training process will be done for 50 epochs, with a batch size of 16. The model will be trained on the dataset of 3296 images, and tested on roughly 500 imanges.\n",
    "* These images are the faces detected using the `face_detection` model, and have been noted to contain atleast 128x128 pixels of face area.\n",
    "* The training will be done on a single GPU, and the model will be saved after every 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139075df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import UNet2DModel, DDPMScheduler\n",
    "from diffusers.optimization import get_cosine_schedule_with_warmup\n",
    "# Since I am unsure of the exact block types and configurations I would need, for now I will use the default DownBlock2d, AttnDownBlock2d and UpBlock2d, AttnUpBlock2d configurations.\n",
    "OUTPUT_DIR=\"model_outputs\"\n",
    "LR_WARMUP_STEPS = 500\n",
    "NUM_EPOCHS = 50\n",
    "\n",
    "\n",
    "model = UNet2DModel(\n",
    "    sample_size=128,\n",
    "    layers_per_block=2, \n",
    ")\n",
    "\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "lr_scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=LR_WARMUP_STEPS,\n",
    "    num_training_steps=(len(train_loader) * NUM_EPOCHS),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bddb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import DDPMPipeline\n",
    "from huggingface_hub import create_repo, upload_folder\n",
    "import wandb\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from pathlib import Path\n",
    "import os\n",
    "import math\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"]=\"WANDB_API_KEY\"\n",
    "\n",
    "\n",
    "def make_grid(images, rows, cols):\n",
    "    w, h = images[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    for i, image in enumerate(images):\n",
    "        grid.paste(image, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "def evaluate(epoch, pipeline):\n",
    "    # Sample some images from random noise (this is the backward diffusion process).\n",
    "    # The default pipeline output type is `List[PIL.Image]`\n",
    "    images = pipeline(\n",
    "        batch_size = 32, \n",
    "        generator=torch.manual_seed(42),\n",
    "    ).images\n",
    "\n",
    "    # Make a grid out of the images\n",
    "    image_grid = make_grid(images, rows=4, cols=4)\n",
    "\n",
    "    # Save the images\n",
    "    test_dir = os.path.join(OUTPUT_DIR, \"samples\")\n",
    "    os.makedirs(test_dir, exist_ok=True)\n",
    "    image_grid.save(f\"{test_dir}/{epoch:04d}.png\")\n",
    "    \n",
    "    # Log to wandb\n",
    "    wandb.log({\n",
    "        \"generated_images\": wandb.Image(image_grid),\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "\n",
    "def train_loop(model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):\n",
    "    # Initialize wandb\n",
    "    wandb.init(\n",
    "        project=\"face-generation\",\n",
    "        name=f\"diffusion_training_{NUM_EPOCHS}epochs\",\n",
    "        dir=OUTPUT_DIR,\n",
    "        notes=\"\",\n",
    "    )\n",
    "    \n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    gradient_accumulation_steps = 2\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    \n",
    "    global_step = 0\n",
    "    accumulated_loss = 0.0\n",
    "    # Training loop\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        model.train()\n",
    "        progress_bar = tqdm(total=len(train_dataloader))\n",
    "        progress_bar.set_description(f\"Epoch {epoch}\")\n",
    "\n",
    "        epoch_loss = 0.0\n",
    "        num_batches=0\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            clean_images = batch['image'].to(device)  # Note: changed from 'images' to 'image' based on your dataset\n",
    "            # Sample noise to add to the images\n",
    "            noise = torch.randn(clean_images.shape, device=device)\n",
    "            bs = clean_images.shape[0]\n",
    "\n",
    "            # Sample a random timestep for each image\n",
    "            timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bs,), device=device).long()\n",
    "\n",
    "            # Add noise to the clean images according to the noise magnitude at each timestep\n",
    "            # (this is the forward diffusion process)\n",
    "            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
    "            \n",
    "            # Predict the noise residual\n",
    "            noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
    "            loss = F.mse_loss(noise_pred, noise)\n",
    "            \n",
    "            loss = loss / gradient_accumulation_steps  # Scale the loss for gradient accumulation\n",
    "            accumulated_loss += loss.item()\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            \n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "                # Gradient clipping\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                actual_loss = accumulated_loss * gradient_accumulation_steps\n",
    "                wandb.log({\n",
    "                    \"train/loss\": actual_loss,\n",
    "                    \"train/learning_rate\": lr_scheduler.get_last_lr()[0],\n",
    "                    \"train/step\": global_step\n",
    "                }, step=global_step)\n",
    "                \n",
    "                logs = {\"loss\": actual_loss, \"lr\": lr_scheduler.get_last_lr()[0], \"step\": global_step}\n",
    "                progress_bar.set_postfix(**logs)\n",
    "                \n",
    "                epoch_loss += actual_loss\n",
    "                accumulated_loss = 0.0\n",
    "                global_step += 1\n",
    "                num_batches += 1\n",
    "            \n",
    "            progress_bar.update(1)\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / num_batches if num_batches > 0 else 0.0\n",
    "        \n",
    "        \n",
    "        # After each epoch you optionally sample some demo images with evaluate() and save the model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
    "\n",
    "            if (epoch + 1) % 2 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                evaluate(epoch, pipeline)\n",
    "\n",
    "            if (epoch + 1) % 2 == 0 or epoch == NUM_EPOCHS - 1:\n",
    "                pipeline.save_pretrained(OUTPUT_DIR)\n",
    "        \n",
    "        # Log epoch metrics\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train/epoch_loss\": avg_epoch_loss  # You might want to track average epoch loss\n",
    "        })\n",
    "    \n",
    "    # Finish wandb run\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276dec0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, noise_scheduler, optimizer, train_loader, lr_scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a844e",
   "metadata": {},
   "source": [
    "## Validation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "629f3ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582 validation images found.\n"
     ]
    }
   ],
   "source": [
    "val_dataset = FaceDataset('data/val')\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "print(val_dataset.__len__(), \"validation images found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8de585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataset_transforms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
